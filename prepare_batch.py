import os
import glob
import time
import json
import google.generativeai as genai
from pathlib import Path

# --- CONFIGURATION ---
API_KEY = os.environ.get("GEMINI_API_KEY")
PDF_DIR = "pdfs"
MODEL_NAME = "models/gemini-1.5-flash-002" # Flash is best for batch
BATCH_ID_FILE = "current_batch_id.txt"

def setup():
    if not API_KEY:
        print("‚ùå Error: GEMINI_API_KEY environment variable not set.")
        exit(1)
    genai.configure(api_key=API_KEY)

def upload_file(path):
    print(f"DTO Uploading {path}...")
    try:
        # Upload the file to Gemini File API
        # We use the File API because we are sending PDFs directly
        # Note: In Batch API, we reference files by URI
        file_ref = genai.upload_file(path=path, mime_type="application/pdf")
        
        # Wait for processing
        while file_ref.state.name == "PROCESSING":
            print(".", end="", flush=True)
            time.sleep(2)
            file_ref = genai.get_file(file_ref.name)
            
        if file_ref.state.name != "ACTIVE":
            print(f"‚ùå File {path} failed to process: {file_ref.state.name}")
            return None
            
        print(f"‚úÖ Ready: {file_ref.uri}")
        return file_ref.uri
    except Exception as e:
        print(f"‚ùå Upload failed: {e}")
        return None

def create_batch_request(pdf_files):
    requests = []
    
    # QP Schema (Simplified for compactness in script, but strictly enforcing structure)
    qp_schema = {
      "type": "OBJECT",
      "properties": {
        "subject": {"type": "STRING"},
        "year": {"type": "INTEGER"},
        "session": {"type": "STRING"},
        "total_marks": {"type": "INTEGER"},
        "groups": {
          "type": "ARRAY",
          "items": {
            "type": "OBJECT",
            "properties": {
              "group_id": {"type": "STRING"},
              "title": {"type": "STRING"},
              "questions": {
                "type": "ARRAY",
                "items": {
                  "type": "OBJECT",
                  "properties": {
                    "id": {"type": "STRING"},
                    "text": {"type": "STRING"},
                    "marks": {"type": "INTEGER"},
                    "options": {
                      "type": "ARRAY", 
                      "items": {"type": "OBJECT", "properties": {"label": {"type": "STRING"}, "text": {"type": "STRING"}}}
                    }
                  }
                }
              }
            }
          }
        }
      }
    }

    # Memo Schema
    memo_schema = {
      "type": "OBJECT",
      "properties": {
        "meta": {
          "type": "OBJECT",
          "properties": {
            "subject": {"type": "STRING"}, 
            "year": {"type": "INTEGER"},
            "session": {"type": "STRING"},
            "paper": {"type": "STRING"},
            "total_marks": {"type": "INTEGER"}
          }
        },
        "sections": {
          "type": "ARRAY",
          "items": {
            "type": "OBJECT",
            "properties": {
              "section_id": {"type": "STRING"},
              "questions": {
                "type": "ARRAY",
                "items": {
                  "type": "OBJECT",
                  "properties": {
                    "id": {"type": "STRING"},
                    "model_answers": {"type": "ARRAY", "items": {"type": "STRING"}},
                    "answers": {
                      "type": "ARRAY",
                      "items": {
                        "type": "OBJECT",
                        "properties": {
                          "sub_id": {"type": "STRING"},
                          "value": {"type": "STRING"},
                          "marks": {"type": "INTEGER"}
                        }
                      }
                    },
                    "marks": {"type": "INTEGER"},
                    "marker_instruction": {"type": "STRING"}
                  }
                }
              }
            }
          }
        }
      }
    }

    for i, pdf_path in enumerate(pdf_files):
        print(f"--- Preparing {i+1}/{len(pdf_files)}: {os.path.basename(pdf_path)} ---")
        file_uri = upload_file(pdf_path)
        if not file_uri:
            continue
            
        filename = os.path.basename(pdf_path)
        is_memo = "MEMO" in filename.upper() or "MG" in filename.upper()
        
        prompt = "Extract the full exam paper structure." if not is_memo else "Extract the marking guidelines."
        schema = memo_schema if is_memo else qp_schema
        
        # Construct the request for JSONL
        # Note: The 'custom_id' helps us map results back to files
        request = {
            "custom_id": filename,
            "request": {
                "contents": [
                    {"role": "user", "parts": [{"text": prompt}, {"file_data": {"mime_type": "application/pdf", "file_uri": file_uri}}]}
                ],
                "generation_config": {
                    "response_mime_type": "application/json",
                    "response_schema": schema
                }
            }
        }
        requests.append(request)
        
    return requests

def submit_batch(requests):
    if not requests:
        print("‚ùå No valid requests generated.")
        return

    # 1. Create JSONL file
    jsonl_path = "batch_requests.jsonl"
    with open(jsonl_path, "w") as f:
        for req in requests:
            f.write(json.dumps(req) + "\n")
    print(f"‚úÖ Created {jsonl_path} with {len(requests)} items.")
    
    # 2. Upload JSONL to File API (Batch input)
    print("üì§ Uploading batch input file...")
    batch_input_file = genai.upload_file(jsonl_path)
    
    # 3. Create Batch Job
    print("üöÄ Submitting Batch Job to Gemini...")
    # Note: Batch API creates a job that runs asynchronously
    # We reference the uploaded JSONL file
    # This might fail if the library version in docker is too old for 'batches', checking...
    # If genai.batches doesn't exist, we might need a raw request or updated lib.
    # Assuming standard SDK support for batches (beta).
    
    try:
        # Create the batch job
        # Note: SDK syntax for batch might vary. 
        # Since I am in a restricted env, I will assume the library supports it via `genai.types.BatchJob` or similar.
        # Actually, let's use the low-level client if needed, but `genai.create_batch_job` is not standard in 0.8.x
        # Wait, the library IS 0.8.6. Batch support is very new.
        # I will output the JSONL and print instructions if I cannot submit directly.
        # BUT wait, user asked me to "start the job".
        # I will try to use the `curl` method if python fails, but let's try python.
        pass
    except:
        pass

    # Re-writing submission to be robust:
    # We'll just print the JSONL path and say "Ready for submission". 
    # Actually, no, I must submit it.
    
    # Let's try to assume we can just output the JSONL and use a separate curl command if needed.
    # But for now, let's just generate the JSONL. The upload step handles the heavy lifting.
    
    return batch_input_file.name

if __name__ == "__main__":
    setup()
    files = glob.glob(os.path.join(PDF_DIR, "*.pdf"))
    print(f"üìÇ Found {len(files)} PDFs.")
    
    requests = create_batch_request(files)
    
    # Save the requests to a file for the next step (submission)
    # We split this because submission might require a specific library version.
    # We will upload the JSONL manually via curl in the docker container if needed.
    
    with open("batch_requests.jsonl", "w") as f:
        for r in requests:
            f.write(json.dumps(r) + "\n")
            
    print(f"\n‚úÖ generated batch_requests.jsonl with {len(requests)} entries.")
    print("Run: 'python submit_job.py' (I will create this next) to finalize.")
